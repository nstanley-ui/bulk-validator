import streamlit as st
import pandas as pd
import os
from mojo_validator.engine import ValidatorEngine
import io
import base64
from pathlib import Path

# Set Page Config
st.set_page_config(
    page_title="Mojo Validator Enterprise v2.0",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for Premium Look
st.markdown("""
    <style>
    .main {
        background-color: #0e1117;
    }
    .stMetric {
        background-color: #1e2130;
        padding: 15px;
        border-radius: 10px;
        border: 1px solid #3e4451;
    }
    .issue-card {
        background-color: #1e2130;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #ff4b4b;
        margin-bottom: 10px;
    }
    .success-card {
        background-color: #1e2130;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #00cc88;
        margin-bottom: 10px;
    }
    .demo-card {
        background-color: #1e2130;
        padding: 20px;
        border-radius: 10px;
        border: 2px solid #4a9eff;
        margin-bottom: 15px;
    }
    .stat-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)

# Initialize Engine
CONFIG_DIR = os.path.join(os.getcwd(), "configs")
os.makedirs(".tmp", exist_ok=True)
engine = ValidatorEngine(CONFIG_DIR)

# --- Session State Management ---
if 'processed_file' not in st.session_state:
    st.session_state.processed_file = None
    st.session_state.raw_df = None
    st.session_state.verified_df = None
    st.session_state.issues = []
    st.session_state.handled = {}  # issue_id -> status
    st.session_state.deleted_rows = set()
    st.session_state.platform = "Unknown"
    st.session_state.severity_filter = "All"

def reset_state():
    for key in ['raw_df', 'verified_df', 'issues', 'handled', 'deleted_rows', 'platform']:
        if key == 'handled': st.session_state[key] = {}
        elif key == 'deleted_rows': st.session_state[key] = set()
        elif key == 'issues': st.session_state[key] = []
        else: st.session_state[key] = None

# --- Action Handlers ---
def handle_fix(issue):
    """Apply suggested fix to verified dataframe."""
    if issue.suggested_fix:
        # Handle smart truncation suggestions (quoted text)
        if issue.suggested_fix.startswith('"') and issue.suggested_fix.endswith('"'):
            fixed_value = issue.suggested_fix[1:-1]  # Remove quotes
            st.session_state.verified_df.at[issue.row_idx, issue.column] = fixed_value
        elif "Change to one of" in issue.suggested_fix:
            # Extract first suggested value
            suggested_values = issue.suggested_fix.split('[')[1].split(']')[0]
            first_value = suggested_values.split(',')[0].strip().strip("'\"")
            st.session_state.verified_df.at[issue.row_idx, issue.column] = first_value
    
    st.session_state.handled[issue.issue_id] = "fixed"

def handle_ignore(issue):
    st.session_state.handled[issue.issue_id] = "ignored"

def handle_remove_row(row_idx):
    st.session_state.deleted_rows.add(row_idx)
    for issue in st.session_state.issues:
        if issue.row_idx == row_idx:
            st.session_state.handled[issue.issue_id] = "removed"

def handle_override(issue, new_value):
    st.session_state.verified_df.at[issue.row_idx, issue.column] = new_value
    st.session_state.handled[issue.issue_id] = "overridden"

def get_download_link(df, filename, file_format="csv"):
    """Generate download link for dataframe."""
    if file_format == "csv":
        csv = df.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        return f'<a href="data:file/csv;base64,{b64}" download="{filename}">Download {filename}</a>'
    else:  # xlsx
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)
        b64 = base64.b64encode(output.getvalue()).decode()
        return f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}">Download {filename}</a>'

# --- Sidebar ---
with st.sidebar:
    st.title("‚öôÔ∏è Configuration")
    
    platform_override = st.selectbox(
        "Platform Override",
        ["Auto-Detect", "LinkedIn Ads", "Google Ads", "Meta Ads"],
        help="Auto-detect analyzes column headers to identify platform"
    )
    override_val = None if platform_override == "Auto-Detect" else platform_override
    
    st.divider()
    
    # Severity Filter
    if st.session_state.issues:
        st.subheader("üîç Filter Issues")
        severity_options = ["All", "BLOCKER", "WARNING"]
        st.session_state.severity_filter = st.radio(
            "Severity",
            severity_options,
            help="Filter issues by severity level"
        )
        
        st.divider()
    
    # Stats
    if st.session_state.issues:
        st.subheader("üìä Quick Stats")
        blockers = sum(1 for i in st.session_state.issues if i.severity == "BLOCKER")
        warnings = sum(1 for i in st.session_state.issues if i.severity == "WARNING")
        
        st.metric("üî¥ Blockers", blockers)
        st.metric("‚ö†Ô∏è Warnings", warnings)
        
        st.divider()
    
    # About
    st.subheader("‚ÑπÔ∏è About")
    st.markdown("""
    **Mojo Validator v2.0**
    
    Production-ready validator with:
    - ‚úÖ Smart truncation
    - ‚úÖ 95%+ validation coverage
    - ‚úÖ 3-5% rejection rate
    
    [GitHub](https://github.com/nstanley-ui/bulk-validator) | [Docs](https://github.com/nstanley-ui/bulk-validator/blob/main/README_UPDATED.md)
    """)

# --- Main Content ---
st.title("üöÄ Mojo Validator Enterprise")
st.markdown("**v2.0** | Bulk-file validation for LinkedIn, Google, and Meta Ads")

# Create tabs for different views
main_tabs = st.tabs(["üì§ Upload & Validate", "üì• Demo Files", "üìñ Quick Start"])

# TAB 1: Upload & Validate
with main_tabs[0]:
    st.subheader("Upload Your Ad File")
    uploaded_file = st.file_uploader(
        "Choose CSV or Excel file",
        type=["csv", "xlsx"],
        help="Upload your bulk ad file for validation"
    )
    
    if uploaded_file:
        if st.session_state.processed_file != uploaded_file.name:
            with st.spinner("üîç Analyzing file..."):
                reset_state()
                tmp_path = os.path.join(".tmp", uploaded_file.name)
                with open(tmp_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                result, verified_df = engine.validate_file(tmp_path, platform_override=override_val)
                st.session_state.raw_df = pd.read_csv(tmp_path) if uploaded_file.name.endswith('.csv') else pd.read_excel(tmp_path)
                st.session_state.verified_df = st.session_state.raw_df.copy()
                st.session_state.issues = result.issues
                st.session_state.platform = result.platform
                st.session_state.processed_file = uploaded_file.name
                st.rerun()
        
        # --- Calculate Metrics ---
        pending_issues = [
            i for i in st.session_state.issues 
            if i.issue_id not in st.session_state.handled 
            and i.row_idx not in st.session_state.deleted_rows
        ]
        
        # Apply severity filter
        if st.session_state.severity_filter != "All":
            pending_issues = [i for i in pending_issues if i.severity == st.session_state.severity_filter]
        
        total_rows = len(st.session_state.raw_df)
        active_rows_count = total_rows - len(st.session_state.deleted_rows)
        rows_with_pending = set(i.row_idx for i in pending_issues)
        clean_active_rows = active_rows_count - len(rows_with_pending)
        
        # --- Display Metrics ---
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("üìÑ Total Rows", total_rows)
        col2.metric("‚úÖ Clean Ads", clean_active_rows, delta=f"{int(clean_active_rows/total_rows*100)}%")
        col3.metric("‚ö†Ô∏è Issues", len(pending_issues))
        col4.metric("üéØ Platform", st.session_state.platform)
        
        st.divider()
        
        # --- Sub Tabs ---
        sub_tabs = st.tabs(["üîç Issues Report", "üìä Data Preview", "üíæ Download"])
        
        # SUB TAB 1: Issues Report
        with sub_tabs[0]:
            if pending_issues:
                st.markdown(f"### Found {len(pending_issues)} issues in {len(rows_with_pending)} ads")
                
                # Progress bar
                progress = len(st.session_state.handled) / max(len(st.session_state.issues), 1)
                st.progress(progress, text=f"Progress: {len(st.session_state.handled)}/{len(st.session_state.issues)} issues handled")
                
                # Group issues by row
                from collections import defaultdict
                grouped_issues = defaultdict(list)
                for issue in pending_issues:
                    grouped_issues[issue.row_idx].append(issue)
                
                # Display issues
                for row_idx in sorted(grouped_issues.keys()):
                    issues_for_row = grouped_issues[row_idx]
                    
                    with st.expander(f"**Row {row_idx + 1}** - {len(issues_for_row)} issue(s)", expanded=True):
                        # Show row data
                        st.markdown("**Current Row Data:**")
                        row_data = st.session_state.verified_df.iloc[row_idx].to_dict()
                        st.json(row_data, expanded=False)
                        
                        st.divider()
                        
                        for issue in issues_for_row:
                            severity_color = "üî¥" if issue.severity == "BLOCKER" else "‚ö†Ô∏è"
                            st.markdown(f"{severity_color} **{issue.column}**: {issue.message}")
                            
                            col_a, col_b = st.columns([2, 1])
                            
                            with col_a:
                                st.text(f"Original: {issue.original_value}")
                                if issue.suggested_fix:
                                    st.text(f"Suggested: {issue.suggested_fix}")
                            
                            with col_b:
                                btn_col1, btn_col2, btn_col3 = st.columns(3)
                                with btn_col1:
                                    if st.button("‚úì Fix", key=f"fix_{issue.issue_id}"):
                                        handle_fix(issue)
                                        st.rerun()
                                with btn_col2:
                                    if st.button("‚Ü∑ Ignore", key=f"ignore_{issue.issue_id}"):
                                        handle_ignore(issue)
                                        st.rerun()
                                with btn_col3:
                                    if st.button("‚úï Remove", key=f"remove_{issue.issue_id}"):
                                        handle_remove_row(row_idx)
                                        st.rerun()
                            
                            # Manual fix input
                            with st.form(key=f"form_{issue.issue_id}"):
                                manual_fix = st.text_input("Enter manual fix:", key=f"manual_{issue.issue_id}")
                                if st.form_submit_button("Apply & Recheck"):
                                    handle_override(issue, manual_fix)
                                    st.rerun()
                            
                            st.divider()
            else:
                st.success("üéâ **All issues resolved!** Your file is ready for upload.")
                st.balloons()
        
        # SUB TAB 2: Data Preview
        with sub_tabs[1]:
            st.subheader("Verified Data Preview")
            
            # Filter options
            show_deleted = st.checkbox("Show deleted rows", value=False)
            
            if show_deleted:
                display_df = st.session_state.verified_df
            else:
                display_df = st.session_state.verified_df.drop(list(st.session_state.deleted_rows))
            
            st.dataframe(display_df, use_container_width=True, height=400)
            
            st.info(f"üìä Showing {len(display_df)} rows (deleted: {len(st.session_state.deleted_rows)})")
        
        # SUB TAB 3: Download
        with sub_tabs[2]:
            st.subheader("üì• Download Results")
            
            # Prepare final dataframe (exclude deleted rows)
            final_df = st.session_state.verified_df.drop(list(st.session_state.deleted_rows))
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.markdown("**üìÑ Download as CSV**")
                csv = final_df.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name=f"validated_{st.session_state.processed_file.replace('.xlsx', '.csv')}",
                    mime="text/csv",
                    use_container_width=True
                )
            
            with col_b:
                st.markdown("**üìä Download as Excel**")
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    final_df.to_excel(writer, index=False, sheet_name='Validated Ads')
                
                st.download_button(
                    label="Download Excel",
                    data=output.getvalue(),
                    file_name=f"validated_{st.session_state.processed_file}",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
            
            st.divider()
            
            # Summary report
            st.markdown("**üìã Validation Summary**")
            summary_text = f"""
            - **Platform Detected**: {st.session_state.platform}
            - **Total Rows**: {total_rows}
            - **Clean Rows**: {clean_active_rows}
            - **Issues Found**: {len(st.session_state.issues)}
            - **Issues Resolved**: {len(st.session_state.handled)}
            - **Rows Deleted**: {len(st.session_state.deleted_rows)}
            """
            st.markdown(summary_text)

# TAB 2: Demo Files
with main_tabs[1]:
    st.subheader("üì• Download Demo Files")
    st.markdown("Don't have an ad file? Try our realistic demo files with 50 ads each.")
    
    # Check if demo files exist
    demo_files = {
        "LinkedIn Ads": {
            "csv": "samples/linkedin_demo_50_realistic.csv",
            "xlsx": "samples/linkedin_demo_50_realistic.xlsx",
            "description": "50 LinkedIn Sponsored Content ads (84% valid, 16% with intentional issues)",
            "issues": "URL format, character limits, invalid status, budget violations"
        },
        "Google Ads": {
            "csv": "samples/google_ads_demo_50_realistic.csv",
            "xlsx": "samples/google_ads_demo_50_realistic.xlsx",
            "description": "50 Google Responsive Search Ads (84% valid, 16% with intentional issues)",
            "issues": "Headlines >30 chars, descriptions >90 chars, missing URLs"
        },
        "Meta Ads": {
            "csv": "samples/meta_ads_demo_50_realistic.csv",
            "xlsx": "samples/meta_ads_demo_50_realistic.xlsx",
            "description": "50 Meta Facebook/Instagram ads (80% valid, 20% with intentional issues)",
            "issues": "Headlines >27 chars (critical!), ALL CAPS, missing URLs"
        }
    }
    
    for platform, files in demo_files.items():
        with st.container():
            st.markdown(f"### {platform}")
            st.markdown(f"*{files['description']}*")
            st.markdown(f"**Sample issues**: {files['issues']}")
            
            col1, col2 = st.columns(2)
            
            # Check if files exist
            csv_exists = os.path.exists(files['csv'])
            xlsx_exists = os.path.exists(files['xlsx'])
            
            with col1:
                if csv_exists:
                    with open(files['csv'], 'rb') as f:
                        st.download_button(
                            label="üìÑ Download CSV",
                            data=f,
                            file_name=os.path.basename(files['csv']),
                            mime="text/csv",
                            use_container_width=True
                        )
                else:
                    st.warning("CSV file not found")
            
            with col2:
                if xlsx_exists:
                    with open(files['xlsx'], 'rb') as f:
                        st.download_button(
                            label="üìä Download Excel",
                            data=f,
                            file_name=os.path.basename(files['xlsx']),
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                else:
                    st.warning("Excel file not found")
            
            st.divider()
    
    st.info("üí° **Tip**: Download a demo file, upload it in the 'Upload & Validate' tab, and see the validator in action!")

# TAB 3: Quick Start
with main_tabs[2]:
    st.subheader("üìñ Quick Start Guide")
    
    st.markdown("""
    ### How to Use the Validator
    
    #### Step 1: Upload or Download Demo
    - **Have a file?** Go to "Upload & Validate" tab
    - **Need a test file?** Go to "Demo Files" tab and download one
    
    #### Step 2: Review Issues
    - The validator will show all issues grouped by row
    - **üî¥ Blockers**: Must fix before upload
    - **‚ö†Ô∏è Warnings**: Recommended to fix
    
    #### Step 3: Fix Issues
    Three ways to fix:
    1. **‚úì Fix**: Apply suggested fix automatically
    2. **Manual Fix**: Enter your own text in the input box
    3. **‚Ü∑ Ignore**: Skip this issue (not recommended for blockers)
    4. **‚úï Remove**: Delete the entire row
    
    #### Step 4: Download
    - Go to "Download" tab
    - Download validated file as CSV or Excel
    - Upload to your ad platform!
    
    ---
    
    ### What Gets Validated?
    
    #### LinkedIn Ads
    - ‚úÖ Character limits (Headline: 200, Introduction: 600)
    - ‚úÖ Required fields (Landing Page URL, Campaign Name)
    - ‚úÖ URL format (http/https required)
    - ‚úÖ Status values (ACTIVE, PAUSED, ARCHIVED)
    - ‚úÖ Budget minimums ($10+)
    
    #### Google Ads
    - ‚úÖ Headlines 1-15 (30 characters each)
    - ‚úÖ Descriptions 1-4 (90 characters each)
    - ‚úÖ Final URL (required, http/https)
    - ‚úÖ Path fields (15 characters each)
    - ‚úÖ Status values (Enabled, Paused, Removed)
    
    #### Meta Ads
    - ‚úÖ Headline (27 characters - critical!)
    - ‚úÖ Primary Text (125 chars visible)
    - ‚úÖ Link Description (30 characters)
    - ‚úÖ Website URL (required, http/https)
    - ‚úÖ Status values (ACTIVE, PAUSED, ARCHIVED)
    
    ---
    
    ### Tips for Best Results
    
    1. **Use Auto-Detect**: Let the validator identify your platform
    2. **Fix Blockers First**: Focus on üî¥ issues before ‚ö†Ô∏è warnings
    3. **Review Suggestions**: Smart truncation shows exactly what text will look like
    4. **Test with Demos**: Try demo files to learn the tool
    5. **Export Often**: Download validated files frequently as you work
    
    ---
    
    ### Need Help?
    
    - üìñ [Full Documentation](https://github.com/nstanley-ui/bulk-validator/blob/main/README_UPDATED.md)
    - üêõ [Report Issues](https://github.com/nstanley-ui/bulk-validator/issues)
    - üìß Contact: support@mojovalidator.com
    """)

# Footer
st.divider()
st.markdown("""
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>Mojo Validator Enterprise v2.0 | Built with ‚ù§Ô∏è for Ad Operations Teams</p>
    <p>Reduces ad rejection rates from 35-45% to 3-5% | 95%+ validation coverage</p>
</div>
""", unsafe_allow_html=True)
